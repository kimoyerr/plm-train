{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install in python < 3.9 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fair-esm\n",
      "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
      "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fair-esm\n",
      "Successfully installed fair-esm-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: fair-esm[esmfold] in /usr/local/lib/python3.8/dist-packages (2.0.0)\n",
      "Collecting biopython (from fair-esm[esmfold])\n",
      "  Downloading biopython-1.83-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting deepspeed==0.5.9 (from fair-esm[esmfold])\n",
      "  Downloading deepspeed-0.5.9.tar.gz (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.3/510.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dm-tree (from fair-esm[esmfold])\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting pytorch-lightning (from fair-esm[esmfold])\n",
      "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting omegaconf (from fair-esm[esmfold])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ml-collections (from fair-esm[esmfold])\n",
      "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from fair-esm[esmfold])\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting scipy (from fair-esm[esmfold])\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (1.24.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.5.9->fair-esm[esmfold]) (1.13.1+cu117)\n",
      "Collecting tqdm (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==1.0.0 (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading triton-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (563 bytes)\n",
      "Collecting absl-py (from ml-collections->fair-esm[esmfold])\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from ml-collections->fair-esm[esmfold]) (6.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from ml-collections->fair-esm[esmfold]) (1.16.0)\n",
      "Collecting contextlib2 (from ml-collections->fair-esm[esmfold])\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->fair-esm[esmfold])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.4.0-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->fair-esm[esmfold]) (4.8.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading aiohttp-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning->fair-esm[esmfold]) (68.2.2)\n",
      "Collecting filelock (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.5.9->fair-esm[esmfold]) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.3.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.3.0-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "  Downloading torch-2.2.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.2.1-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "  Downloading torch-2.2.0-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "  Downloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.1.1-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "INFO: pip is still looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting torch (from deepspeed==0.5.9->fair-esm[esmfold])\n",
      "  Downloading torch-2.0.0-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting pytorch-lightning (from fair-esm[esmfold])\n",
      "  Downloading pytorch_lightning-2.3.2-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pytorch_lightning-2.3.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl.metadata (21 kB)\n",
      "  Downloading pytorch_lightning-2.2.5-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading aiohappyeyeballs-2.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold]) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold])\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->fair-esm[esmfold]) (3.4)\n",
      "Downloading triton-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading biopython-1.83-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.2.5-py3-none-any.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m158.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m189.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading aiohttp-3.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m184.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.3.2-py3-none-any.whl (11 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.8/308.8 kB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed, ml-collections, antlr4-python3-runtime\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.5.9-py3-none-any.whl size=524316 sha256=5fcf3e909f624369ac52c717b7f029fa0dd30870c6212c33d5d42e875ab96b52\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/41/fc/116ba18dcc5a75476d24b65604a2faaa21c0be0f5784f3ef25\n",
      "  Building wheel for ml-collections (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94507 sha256=38a185e0c2d1639b42270d0538cf739adb4302063fea30a26b43dd0ace6a3729\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/9f/a9/9e8309035a5bf09ed9086bbca8c9b74cb6413d3eb203e2bc8c\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=15b7585ca8aa1cb4a8173f6669a57bd1a85c1d28a64798aa030e7fc17e0d29fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "Successfully built deepspeed ml-collections antlr4-python3-runtime\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, dm-tree, antlr4-python3-runtime, tqdm, scipy, omegaconf, multidict, lightning-utilities, fsspec, frozenlist, einops, contextlib2, biopython, async-timeout, aiohappyeyeballs, absl-py, yarl, triton, torchmetrics, ml-collections, aiosignal, deepspeed, aiohttp, pytorch-lightning\n",
      "Successfully installed absl-py-2.1.0 aiohappyeyeballs-2.3.2 aiohttp-3.10.0 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.3 biopython-1.83 contextlib2-21.6.0 deepspeed-0.5.9 dm-tree-0.1.8 einops-0.8.0 frozenlist-1.4.1 fsspec-2024.6.1 hjson-3.1.0 lightning-utilities-0.11.6 ml-collections-0.1.1 multidict-6.0.5 ninja-1.11.1.1 omegaconf-2.3.0 py-cpuinfo-9.0.0 pytorch-lightning-2.2.5 scipy-1.10.1 torchmetrics-1.4.0.post0 tqdm-4.66.4 triton-1.0.0 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting dllogger@ git+https://github.com/NVIDIA/dllogger.git\n",
      "  Cloning https://github.com/NVIDIA/dllogger.git to /tmp/pip-install-mkf831q1/dllogger_0df9622650af44f3a58557feb808d7b4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger.git /tmp/pip-install-mkf831q1/dllogger_0df9622650af44f3a58557feb808d7b4\n",
      "  Resolved https://github.com/NVIDIA/dllogger.git to commit 0540a43971f4a8a16693a9de9de73c1072020769\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: dllogger\n",
      "  Building wheel for dllogger (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dllogger: filename=DLLogger-1.0.0-py3-none-any.whl size=5654 sha256=feab7b362207ce71b202b065ecc62491aa8b95e7811d53d382fc6bd4d95b0f68\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nwdjot7f/wheels/c4/f9/e7/05e0371c5078725c26b6251a30b873f061a4387de03d3fcbcb\n",
      "Successfully built dllogger\n",
      "Installing collected packages: dllogger\n",
      "Successfully installed dllogger-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting openfold@ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307\n",
      "  Cloning https://github.com/aqlaboratory/openfold.git (to revision 4b41059694619831a7db195b7e0988fc4ff3a307) to /tmp/pip-install-ptf86d8x/openfold_f230c0f826324acfa2df2764bbcef5e5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aqlaboratory/openfold.git /tmp/pip-install-ptf86d8x/openfold_f230c0f826324acfa2df2764bbcef5e5\n",
      "  Running command git rev-parse -q --verify 'sha^4b41059694619831a7db195b7e0988fc4ff3a307'\n",
      "  Running command git fetch -q https://github.com/aqlaboratory/openfold.git 4b41059694619831a7db195b7e0988fc4ff3a307\n",
      "  Running command git checkout -q 4b41059694619831a7db195b7e0988fc4ff3a307\n",
      "  Resolved https://github.com/aqlaboratory/openfold.git to commit 4b41059694619831a7db195b7e0988fc4ff3a307\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: openfold\n",
      "  Building wheel for openfold (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openfold: filename=openfold-1.0.0-cp38-cp38-linux_x86_64.whl size=2674536 sha256=72c496460d40efd2f93cbc543c527d2b3eb9249d9332cb638d988cb145f81866\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/e4/59/4ca275bd404099dc825479b6f77e81b8443afbc04f960ce616\n",
      "Successfully built openfold\n",
      "Installing collected packages: openfold\n",
      "Successfully installed openfold-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting biotite\n",
      "  Downloading biotite-0.40.0.tar.gz (22.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.12 in /usr/local/lib/python3.8/dist-packages (from biotite) (2.31.0)\n",
      "Requirement already satisfied: numpy<=2.0,>=1.14.5 in /usr/local/lib/python3.8/dist-packages (from biotite) (1.24.4)\n",
      "Collecting msgpack>=0.5.6 (from biotite)\n",
      "  Downloading msgpack-1.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting networkx>=2.0 (from biotite)\n",
      "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12->biotite) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12->biotite) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12->biotite) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.12->biotite) (2023.7.22)\n",
      "Downloading msgpack-1.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.5/390.5 kB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m190.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: biotite\n",
      "  Building wheel for biotite (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for biotite: filename=biotite-0.40.0-cp38-cp38-linux_x86_64.whl size=41691166 sha256=5ace6cd6ab74126a046420c34f536efcdd41b69dcf7b98566ea71a97e877583e\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/23/fe/3f242aaa74e188d986f5e1a2038d84fab62478d1730db8616a\n",
      "Successfully built biotite\n",
      "Installing collected packages: networkx, msgpack, biotite\n",
      "Successfully installed biotite-0.40.0 msgpack-1.0.8 networkx-3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pytorch-lightning==1.8.4\n",
      "  Downloading pytorch_lightning-1.8.4-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (1.13.1+cu117)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (2024.6.1)\n",
      "Collecting tensorboardX>=2.2 (from pytorch-lightning==1.8.4)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (1.4.0.post0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (4.8.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.8.4) (0.11.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (3.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities!=0.4.0,>=0.3.0->pytorch-lightning==1.8.4) (68.2.2)\n",
      "Collecting protobuf>=3.20 (from tensorboardX>=2.2->pytorch-lightning==1.8.4)\n",
      "  Downloading protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (2.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.4) (3.4)\n",
      "Downloading pytorch_lightning-1.8.4-py3-none-any.whl (799 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.0/800.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, tensorboardX, pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.5\n",
      "    Uninstalling pytorch-lightning-2.2.5:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.5\n",
      "Successfully installed protobuf-5.27.2 pytorch-lightning-1.8.4 tensorboardX-2.6.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fair-esm\n",
    "!pip install \"fair-esm[esmfold]\"\n",
    "!pip install 'dllogger @ git+https://github.com/NVIDIA/dllogger.git'\n",
    "!pip install 'openfold @ git+https://github.com/aqlaboratory/openfold.git@4b41059694619831a7db195b7e0988fc4ff3a307'\n",
    "!pip install biotite\n",
    "!pip install pytorch-lightning==1.8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "import esm\n",
    "import biotite.structure.io as bsio\n",
    "import biotite.structure as struc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/workspace/plm-train/data/netsolp/PSI_Biology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt\" to /root/.cache/torch/hub/checkpoints/esmfold_3B_v1.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t36_3B_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t36_3B_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t36_3B_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t36_3B_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "model = esm.pretrained.esmfold_v1()\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data: pET_full_without_his_tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmppfrzm8am\n"
     ]
    }
   ],
   "source": [
    "# Read fasta file and iterate through all sequences\n",
    "fasta_file = os.path.join(data_dir, \"pET_full_without_his_tag.fa\")\n",
    "fasta_sequences = SeqIO.parse(open(fasta_file), \"fasta\")\n",
    "\n",
    "\n",
    "# Temporary directory to store the pdb files\n",
    "protein_names = []\n",
    "protein_sasa = []\n",
    "chunk_counts = 0\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    print('Created temporary directory:', tmpdirname)\n",
    "    # Iterate through all the fasta sequences\n",
    "    for fasta in fasta_sequences:\n",
    "        sel_name, sel_seq = fasta.id, str(fasta.seq)\n",
    "        with torch.no_grad():\n",
    "            output = model.infer_pdb(sel_seq)\n",
    "        tmp_pdb_file = os.path.join(tmpdirname, \"result.pdb\")\n",
    "        with open(tmp_pdb_file, \"w\") as f:\n",
    "            f.write(output)\n",
    "\n",
    "        atom_array = bsio.load_structure(tmp_pdb_file)\n",
    "        atom_array = atom_array[atom_array.hetero == False]\n",
    "        assert len(np.unique(atom_array.res_id)) == len(sel_seq)\n",
    "        assert np.array_equal(\n",
    "            np.sort(np.unique(atom_array.res_id)), np.arange(1, atom_array.res_id.max() + 1)\n",
    "        )\n",
    "        atom_sasa = struc.sasa(atom_array)\n",
    "        # Sum up SASA for each residue in chain\n",
    "        # res_sasa_chain = struc.apply_residue_wise(sel_chain, atom_sasa_chain, np.sum)\n",
    "        res_sasa = np.bincount(atom_array.res_id,  weights=atom_sasa)[1:]  # From ESM3\n",
    "        assert res_sasa.shape[0] == len(sel_seq)\n",
    "        \n",
    "        # Add to list\n",
    "        protein_names.append(sel_name)\n",
    "        protein_sasa.append(res_sasa)\n",
    "\n",
    "        # Save periodic results after every 500 proteins\n",
    "        chunk_size = 100\n",
    "        if len(protein_names) % chunk_size == 0:\n",
    "            suffix_name_1 = str(chunk_counts + int(np.floor(len(protein_names)/chunk_size) - 1))\n",
    "            suffix_name_2 = str(chunk_counts + int(np.floor(len(protein_names)/chunk_size)))\n",
    "            npz_file_name = os.path.join(data_dir, \"sasa_data\", f\"pET_full_without_his_tag_sasa_data_{suffix_name_1}_{suffix_name_2}.npz\")\n",
    "            np.savez(npz_file_name, **dict(zip(protein_names, protein_sasa)))\n",
    "\n",
    "            # Reset the lists\n",
    "            protein_names = []\n",
    "            protein_sasa = []\n",
    "            chunk_counts+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate all the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "# Read through all the npz chunks and consolidate into one npz file\n",
    "npz_files = os.listdir(os.path.join(data_dir, \"sasa_data\"))\n",
    "print(len(npz_files))\n",
    "protein_names = []\n",
    "protein_sasa = []\n",
    "for npz_file in npz_files:\n",
    "    data = np.load(os.path.join(data_dir, \"sasa_data\", npz_file))\n",
    "    protein_names.extend(list(data.keys()))\n",
    "    protein_sasa.extend(list(data.values()))\n",
    "\n",
    "# save the consolidated data\n",
    "np.savez(os.path.join(data_dir, \"pET_full_without_his_tag_sasa_data_consolidated.npz\"), **dict(zip(protein_names, protein_sasa)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12200\n"
     ]
    }
   ],
   "source": [
    "# Load the consolidated data\n",
    "data = np.load(os.path.join(data_dir, \"pET_full_without_his_tag_sasa_data_consolidated.npz\"))\n",
    "protein_names = list(data.keys())\n",
    "protein_sasa = list(data.values())\n",
    "print(len(protein_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fasta file and iterate through all sequences\n",
    "fasta_file = os.path.join(data_dir, \"pET_full_without_his_tag.fa\")\n",
    "fasta_sequences = SeqIO.parse(open(fasta_file), \"fasta\")\n",
    "\n",
    "# Convert fasta_sequences to dictionary\n",
    "fasta_dict = {}\n",
    "for fasta in fasta_sequences:\n",
    "    fasta_dict[fasta.id] = str(fasta.seq)\n",
    "\n",
    "# Create a new fasta file with the fasta ids of protein_names\n",
    "new_fasta_file = os.path.join(data_dir, \"pET_full_without_his_tag_consolidated.fasta\")\n",
    "with open(new_fasta_file, \"w\") as f:\n",
    "    for protein_name in protein_names:\n",
    "        f.write(f\">{protein_name}\\n{fasta_dict[protein_name]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data: NESG_Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary directory: /tmp/tmpbfyvk7xu\n"
     ]
    }
   ],
   "source": [
    "# Read fasta file and iterate through all sequences\n",
    "fasta_file = os.path.join(data_dir, \"NESG_testset.fasta\")\n",
    "fasta_sequences = SeqIO.parse(open(fasta_file), \"fasta\")\n",
    "\n",
    "\n",
    "# Temporary directory to store the pdb files\n",
    "protein_names = []\n",
    "protein_sasa = []\n",
    "failed_sequences = []\n",
    "chunk_counts = 0\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    print('Created temporary directory:', tmpdirname)\n",
    "    # Iterate through all the fasta sequences\n",
    "    for fasta in fasta_sequences:\n",
    "        sel_name, sel_seq = fasta.id, str(fasta.seq)\n",
    "        with torch.no_grad():\n",
    "            output = model.infer_pdb(sel_seq)\n",
    "        tmp_pdb_file = os.path.join(tmpdirname, \"result.pdb\")\n",
    "        with open(tmp_pdb_file, \"w\") as f:\n",
    "            f.write(output)\n",
    "\n",
    "        atom_array = bsio.load_structure(tmp_pdb_file)\n",
    "        atom_array = atom_array[atom_array.hetero == False]\n",
    "        if len(np.unique(atom_array.res_id)) != len(sel_seq):\n",
    "            # Go to next fasta\n",
    "            failed_sequences.append(sel_seq)\n",
    "            continue\n",
    "        assert len(np.unique(atom_array.res_id)) == len(sel_seq)\n",
    "        assert np.array_equal(\n",
    "            np.sort(np.unique(atom_array.res_id)), np.arange(1, atom_array.res_id.max() + 1)\n",
    "        )\n",
    "        atom_sasa = struc.sasa(atom_array)\n",
    "        # Sum up SASA for each residue in chain\n",
    "        # res_sasa_chain = struc.apply_residue_wise(sel_chain, atom_sasa_chain, np.sum)\n",
    "        res_sasa = np.bincount(atom_array.res_id,  weights=atom_sasa)[1:]  # From ESM3\n",
    "        assert res_sasa.shape[0] == len(sel_seq)\n",
    "        \n",
    "        # Add to list\n",
    "        protein_names.append(sel_name)\n",
    "        protein_sasa.append(res_sasa)\n",
    "\n",
    "        # Save periodic results after every 500 proteins\n",
    "        chunk_size = 100\n",
    "        if len(protein_names) % chunk_size == 0:\n",
    "            suffix_name_1 = str(chunk_counts + int(np.floor(len(protein_names)/chunk_size) - 1))\n",
    "            suffix_name_2 = str(chunk_counts + int(np.floor(len(protein_names)/chunk_size)))\n",
    "            npz_file_name = os.path.join(data_dir, \"sasa_data_NESG_testset\", f\"NESG_testset_sasa_data_{suffix_name_1}_{suffix_name_2}.npz\")\n",
    "            np.savez(npz_file_name, **dict(zip(protein_names, protein_sasa)))\n",
    "\n",
    "            # Reset the lists\n",
    "            protein_names = []\n",
    "            protein_sasa = []\n",
    "            chunk_counts+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Read through all the npz chunks and consolidate into one npz file\n",
    "npz_files = os.listdir(os.path.join(data_dir, \"sasa_data_NESG_testset\"))\n",
    "print(len(npz_files))\n",
    "protein_names = []\n",
    "protein_sasa = []\n",
    "for npz_file in npz_files:\n",
    "    data = np.load(os.path.join(data_dir, \"sasa_data_NESG_testset\", npz_file))\n",
    "    protein_names.extend(list(data.keys()))\n",
    "    protein_sasa.extend(list(data.values()))\n",
    "\n",
    "# save the consolidated data\n",
    "np.savez(os.path.join(data_dir, \"NESG_testset_sasa_data_consolidated.npz\"), **dict(zip(protein_names, protein_sasa)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    }
   ],
   "source": [
    "# Load the consolidated data\n",
    "data = np.load(os.path.join(data_dir, \"NESG_testset_sasa_data_consolidated.npz\"))\n",
    "protein_names = list(data.keys())\n",
    "protein_sasa = list(data.values())\n",
    "print(len(protein_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read fasta file and iterate through all sequences\n",
    "fasta_file = os.path.join(data_dir, \"NESG_testset.fasta\")\n",
    "fasta_sequences = SeqIO.parse(open(fasta_file), \"fasta\")\n",
    "\n",
    "# Convert fasta_sequences to dictionary\n",
    "fasta_dict = {}\n",
    "for fasta in fasta_sequences:\n",
    "    fasta_dict[fasta.id] = str(fasta.seq)\n",
    "\n",
    "# Create a new fasta file with the fasta ids of protein_names\n",
    "new_fasta_file = os.path.join(data_dir, \"NESG_testset_sasa_data_consolidated.fasta\")\n",
    "with open(new_fasta_file, \"w\") as f:\n",
    "    for protein_name in protein_names:\n",
    "        f.write(f\">{protein_name}\\n{fasta_dict[protein_name]}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
